{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json \n",
    "from PIL import Image, ImageDraw\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from transformers.image_transforms import center_to_corners_format\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = \"/home/taheera.ahmed/data/reindeerdrone/tiles/new_annotations.json\"\n",
    "data_dir = \"/home/taheera.ahmed/data/reindeerdrone/tiles\"\n",
    "img_dir = \"/home/taheera.ahmed/data/reindeerdrone/tiles/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
       "    num_rows: 251\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datasets = load_dataset('json', data_files=annotation_file, field='annotations')\n",
    "\n",
    "def process_image_id(image_id):\n",
    "    # Remove the 'DSC' and '_tile' parts\n",
    "    stripped_id = image_id.replace(\"DSC\", \"\").replace(\"_tile\", \"\").replace(\".png\", \"\")\n",
    "    return int(stripped_id)\n",
    "\n",
    "def combine_bboxes(examples):\n",
    "    # Use defaultdict to initialize combined data for each image_id\n",
    "    combined = defaultdict(lambda: {\n",
    "        \"image_id\": None,\n",
    "        \"image\": None,\n",
    "        \"width\": None,\n",
    "        \"height\": None,\n",
    "        \"objects\": {\n",
    "            'id': [], \n",
    "            'area': [],\n",
    "            'category': [],\n",
    "            'bbox': []\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Loop over each example to combine bounding boxes by image_id\n",
    "    for count, example in enumerate(examples):\n",
    "        image_id = example['image_id']\n",
    "        bbox = example['bbox']\n",
    "        area = example['area']\n",
    "        category_id = example['category_id']\n",
    "\n",
    "        # Add bbox, category, and area to the corresponding image entry\n",
    "        combined[image_id][\"objects\"][\"bbox\"].append(bbox)\n",
    "        combined[image_id][\"objects\"][\"category\"].append(category_id)\n",
    "        combined[image_id][\"objects\"][\"area\"].append(area)\n",
    "        combined[image_id][\"objects\"][\"id\"].append(count)\n",
    "        combined[image_id][\"image_id\"] = image_id\n",
    "        \n",
    "        # Load the image using PIL and get its dimensions\n",
    "        image_path = os.path.join(img_dir, image_id)\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        \n",
    "        combined[image_id][\"image\"] = image\n",
    "        combined[image_id][\"width\"] = width\n",
    "        combined[image_id][\"height\"] = height\n",
    "    \n",
    "    # Flatten combined data into a list of dictionaries, one for each image\n",
    "    combined_data = []\n",
    "    for image_id, data in combined.items():\n",
    "        combined_data.append({\n",
    "            \"image_id\": process_image_id(data[\"image_id\"]),\n",
    "            #\"image_id\": data[\"image_id\"],\n",
    "            \"image\": data[\"image\"],\n",
    "            \"width\": data[\"width\"],\n",
    "            \"height\": data[\"height\"],\n",
    "            \"objects\": data[\"objects\"]\n",
    "        })\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "combined_annotations = combine_bboxes(datasets['train'])\n",
    "dataset = Dataset.from_list(combined_annotations)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Dataset({\n",
      "    features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
      "    num_rows: 213\n",
      "})\n",
      "Validation Set:\n",
      "Dataset({\n",
      "    features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
      "    num_rows: 38\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_id': 54735,\n",
       " 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1024x1024>,\n",
       " 'width': 1024,\n",
       " 'height': 1024,\n",
       " 'objects': {'area': [12957.285532965,\n",
       "   2582.2501702644,\n",
       "   10277.7077077713,\n",
       "   6468.324913521],\n",
       "  'bbox': [[7.5581192042000005, 908.4531502426, 143.1929283019, 90.4883061379],\n",
       "   [7.1121505028, 947.2585858125, 59.9725403355, 43.057208446],\n",
       "   [615.6783560146, 946.6784266368, 132.9216059728, 77.3215733632],\n",
       "   [676.1825675824, 955.737002753, 94.7559464773, 68.262997247]],\n",
       "  'category': [0, 1, 0, 1],\n",
       "  'id': [410, 411, 412, 413]}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = dataset.train_test_split(test_size=0.15, seed=1337)\n",
    "\n",
    "dataset = {\n",
    "    'train': split['train'],\n",
    "    'validation': split['test']\n",
    "}\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(dataset['train'])\n",
    "print(\"Validation Set:\")\n",
    "print(dataset['validation'])\n",
    "\n",
    "dataset[\"train\"][2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'name': 'Adult'}, {'id': 1, 'name': 'Calf'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read json file and get categories \n",
    "with open(annotation_file, 'r') as file:\n",
    "    annotations = json.load(file)\n",
    "    \n",
    "categories = annotations['categories']\n",
    "categories = [{\"id\": category['id'], \"name\": category['name']} for category in categories]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = dataset[\"train\"][2][\"objects\"][\"bbox\"]\n",
    "image = dataset[\"train\"][2][\"image\"]\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for box in annotations:\n",
    "    xmin, ymin, width, height = box\n",
    "    xmax = xmin + width\n",
    "    ymax = ymin + height\n",
    "    draw.rectangle([xmin, ymin, xmax, ymax], outline=\"red\", width=3)\n",
    "\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "MAX_SIZE = 1024\n",
    "MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    do_resize=True,\n",
    "    size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE},\n",
    "    do_pad=True,\n",
    "    pad_size={\"height\": MAX_SIZE, \"width\": MAX_SIZE},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_augment_and_transform = A.Compose(\n",
    "    [\n",
    "        #A.Perspective(p=0.1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        #A.RandomBrightnessContrast(p=0.5),\n",
    "        #A.HueSaturationValue(p=0.1),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n",
    ")\n",
    "\n",
    "validation_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n",
    ")\n",
    "\n",
    "def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n",
    "    \"\"\"Format one set of image annotations to the COCO format\n",
    "\n",
    "    Args:\n",
    "        image_id (str): image id. e.g. \"0001\"\n",
    "        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n",
    "        areas (List[float]): list of corresponding areas to provided bounding boxes\n",
    "        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n",
    "            ([center_x, center_y, width, height] in absolute coordinates)\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"image_id\": image id,\n",
    "            \"annotations\": list of formatted annotations\n",
    "        }\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    for category, area, bbox in zip(categories, areas, bboxes):\n",
    "        formatted_annotation = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category,\n",
    "            \"iscrowd\": 0,\n",
    "            \"area\": area,\n",
    "            \"bbox\": list(bbox),\n",
    "        }\n",
    "        annotations.append(formatted_annotation)\n",
    "\n",
    "    return {\n",
    "        \"image_id\": image_id,\n",
    "        \"annotations\": annotations,\n",
    "    }\n",
    "\n",
    "def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n",
    "    \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        # apply augmentations\n",
    "        output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "        images.append(output[\"image\"])\n",
    "\n",
    "        # format annotations in COCO format\n",
    "        formatted_annotations = format_image_annotations_as_coco(\n",
    "            image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n",
    "        )\n",
    "        annotations.append(formatted_annotations)\n",
    "\n",
    "    # Apply the image processor transformations: resizing, rescaling, normalization\n",
    "    result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "\n",
    "    if not return_pixel_mask:\n",
    "        result.pop(\"pixel_mask\", None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-0.3541, -0.3369, -0.4226,  ..., -0.4397, -0.3541, -0.3027],\n",
       "          [-0.4397, -0.4568, -0.6109,  ..., -0.4568, -0.3541, -0.2856],\n",
       "          [-0.6281, -0.6965, -0.8335,  ..., -0.5596, -0.5253, -0.4911],\n",
       "          ...,\n",
       "          [ 0.5707,  0.6563,  0.9303,  ...,  0.3823,  0.1768,  0.0912],\n",
       "          [ 0.5536,  0.6392,  0.8789,  ...,  0.3138,  0.1254,  0.1768],\n",
       "          [ 0.5193,  0.5536,  0.6734,  ...,  0.1597, -0.1828, -0.0458]],\n",
       " \n",
       "         [[-0.2150, -0.1975, -0.2325,  ..., -0.3200, -0.3200, -0.2850],\n",
       "          [-0.3025, -0.3200, -0.4601,  ..., -0.3375, -0.3025, -0.2325],\n",
       "          [-0.4776, -0.5476, -0.6877,  ..., -0.4426, -0.4251, -0.3901],\n",
       "          ...,\n",
       "          [ 0.5728,  0.6604,  0.9405,  ...,  0.6254,  0.4153,  0.3277],\n",
       "          [ 0.5728,  0.6604,  0.8880,  ...,  0.5553,  0.3627,  0.4153],\n",
       "          [ 0.5378,  0.5728,  0.7304,  ...,  0.4328,  0.0476,  0.1877]],\n",
       " \n",
       "         [[-0.5495, -0.5321, -0.6193,  ..., -0.5495, -0.4798, -0.4450],\n",
       "          [-0.6715, -0.6890, -0.8807,  ..., -0.5670, -0.5147, -0.4450],\n",
       "          [-0.9330, -1.0027, -1.1421,  ..., -0.7064, -0.6890, -0.6541],\n",
       "          ...,\n",
       "          [ 0.1128,  0.1999,  0.5136,  ...,  0.0431, -0.1661, -0.2881],\n",
       "          [ 0.0256,  0.1128,  0.4265,  ..., -0.0267, -0.2184, -0.2010],\n",
       "          [ 0.0256,  0.0605,  0.2522,  ..., -0.1661, -0.5321, -0.4275]]]),\n",
       " 'labels': {'size': tensor([1024, 1024]), 'image_id': tensor([54735]), 'class_labels': tensor([0, 1, 0, 1]), 'boxes': tensor([[0.9227, 0.9313, 0.1398, 0.0884],\n",
       "         [0.9638, 0.9461, 0.0586, 0.0420],\n",
       "         [0.3338, 0.9622, 0.1298, 0.0755],\n",
       "         [0.2934, 0.9667, 0.0925, 0.0667]]), 'area': tensor([12957.2852,  2582.2502, 10277.7080,  6468.3247]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1024, 1024])}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n",
    ")\n",
    "validation_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n",
    ")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_batch)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].with_transform(validation_transform_batch)\n",
    "\n",
    "dataset[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing function to compute mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n",
    "    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): Bounding boxes in YOLO format\n",
    "        image_size (Tuple[int, int]): Image size in format (height, width)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # convert center to corners format\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    # convert to absolute coordinates\n",
    "    height, width = image_size\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "    \n",
    "@torch.no_grad()\n",
    "def compute_average_precision(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
    "    \"\"\"\n",
    "    Compute Average Precision (AP) for each class in an object detection task.\n",
    "\n",
    "    Args:\n",
    "        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
    "        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n",
    "        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Mapping[str, float]: Metrics in a form of dictionary {<class_name>: <AP_value>}\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "    image_sizes = []\n",
    "    post_processed_targets = []\n",
    "    post_processed_predictions = []\n",
    "\n",
    "    # Collect targets in the required format for metric computation\n",
    "    for batch in targets:\n",
    "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
    "        image_sizes.append(batch_image_sizes)\n",
    "        for image_target in batch:\n",
    "            boxes = torch.tensor(image_target[\"boxes\"])\n",
    "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
    "            labels = torch.tensor(image_target[\"class_labels\"])\n",
    "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "    # Collect predictions in the required format for metric computation\n",
    "    for batch, target_sizes in zip(predictions, image_sizes):\n",
    "        batch_logits, batch_boxes = batch[1], batch[2]\n",
    "        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "        post_processed_output = image_processor.post_process_object_detection(\n",
    "            output, threshold=threshold, target_sizes=target_sizes\n",
    "        )\n",
    "        post_processed_predictions.extend(post_processed_output)\n",
    "\n",
    "    # Compute metrics using MeanAveragePrecision (this will provide class-level AP)\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "    metric.update(post_processed_predictions, post_processed_targets)\n",
    "    metrics = metric.compute()\n",
    "\n",
    "    # Extract per-class AP values\n",
    "    classes = metrics.pop(\"classes\")\n",
    "    ap_per_class = metrics.pop(\"map_per_class\")  # This gives AP per class\n",
    "\n",
    "    # Format results as {class_name: AP_value}\n",
    "    ap_results = {}\n",
    "    ap_values = []\n",
    "    for class_id, class_ap in zip(classes, ap_per_class):\n",
    "        class_name = id2label[class_id.item()] if id2label is not None else f\"class_{class_id.item()}\"\n",
    "        ap_results[f\"ap_{class_name}\"] = round(class_ap.item(), 4)\n",
    "        ap_values.append(class_ap.item())  # Collect AP values\n",
    "\n",
    "    # Calculate general AP (mean AP across all classes)\n",
    "    ap_results[\"eval_ap\"] = sum(ap_values) / len(ap_values)\n",
    "\n",
    "    return ap_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {category[\"id\"]: category[\"name\"] for category in categories}\n",
    "label2id = {category[\"name\"]: category[\"id\"] for category in categories}\n",
    "\n",
    "eval_compute_metrics_fn = partial(\n",
    "    compute_average_precision, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1900957:7:0927/103112.433734:ERROR:command_buffer_proxy_impl.cc(132)] ContextResult::kTransientFailure: Failed to send GpuControl.CreateCommandBuffer.\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr_finetuned_reindeerdrone\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    dataloader_num_workers=4,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=1e-4,\n",
    "    max_grad_norm=0.01,\n",
    "    metric_for_best_model=\"eval_ap\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/810 [14:18<6:55:04, 31.81s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                             \n",
      "  3%|▎         | 27/810 [04:54<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.0053120015654712915, 'eval_loss': 1.9330289363861084, 'eval_ap_Adult': 0.0061, 'eval_ap_Calf': 0.0045, 'eval_runtime': 2.8081, 'eval_samples_per_second': 13.532, 'eval_steps_per_second': 1.781, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                             \n",
      "  3%|▎         | 27/810 [05:11<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.01002857880666852, 'eval_loss': 1.9049028158187866, 'eval_ap_Adult': 0.0141, 'eval_ap_Calf': 0.006, 'eval_runtime': 2.9238, 'eval_samples_per_second': 12.997, 'eval_steps_per_second': 1.71, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                             \n",
      "  3%|▎         | 27/810 [05:27<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.01703260187059641, 'eval_loss': 1.8919119834899902, 'eval_ap_Adult': 0.024, 'eval_ap_Calf': 0.0101, 'eval_runtime': 2.807, 'eval_samples_per_second': 13.538, 'eval_steps_per_second': 1.781, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [05:44<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.016353588551282883, 'eval_loss': 1.8861632347106934, 'eval_ap_Adult': 0.0217, 'eval_ap_Calf': 0.011, 'eval_runtime': 2.8864, 'eval_samples_per_second': 13.165, 'eval_steps_per_second': 1.732, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [06:00<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.02145358733832836, 'eval_loss': 1.8292330503463745, 'eval_ap_Adult': 0.0429, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.7491, 'eval_samples_per_second': 13.822, 'eval_steps_per_second': 1.819, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [06:17<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.012529902160167694, 'eval_loss': 1.8950300216674805, 'eval_ap_Adult': 0.0251, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.7984, 'eval_samples_per_second': 13.579, 'eval_steps_per_second': 1.787, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [06:33<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.008339914493262768, 'eval_loss': 2.131045341491699, 'eval_ap_Adult': 0.0167, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8844, 'eval_samples_per_second': 13.174, 'eval_steps_per_second': 1.733, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [06:50<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.022186482325196266, 'eval_loss': 1.8248848915100098, 'eval_ap_Adult': 0.0444, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8245, 'eval_samples_per_second': 13.454, 'eval_steps_per_second': 1.77, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [07:07<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.02563758217729628, 'eval_loss': 1.7182996273040771, 'eval_ap_Adult': 0.0506, 'eval_ap_Calf': 0.0007, 'eval_runtime': 2.8184, 'eval_samples_per_second': 13.483, 'eval_steps_per_second': 1.774, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [07:23<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.043873610207811, 'eval_loss': 1.703630805015564, 'eval_ap_Adult': 0.0871, 'eval_ap_Calf': 0.0007, 'eval_runtime': 2.8539, 'eval_samples_per_second': 13.315, 'eval_steps_per_second': 1.752, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [07:40<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.03462309390306473, 'eval_loss': 1.6803460121154785, 'eval_ap_Adult': 0.0692, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.9132, 'eval_samples_per_second': 13.044, 'eval_steps_per_second': 1.716, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [07:56<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.02890661545097828, 'eval_loss': 1.6154876947402954, 'eval_ap_Adult': 0.0578, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8092, 'eval_samples_per_second': 13.527, 'eval_steps_per_second': 1.78, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [08:13<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.03201860189437866, 'eval_loss': 1.5900242328643799, 'eval_ap_Adult': 0.064, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8614, 'eval_samples_per_second': 13.28, 'eval_steps_per_second': 1.747, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [08:30<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.03248976171016693, 'eval_loss': 1.5625640153884888, 'eval_ap_Adult': 0.065, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8734, 'eval_samples_per_second': 13.225, 'eval_steps_per_second': 1.74, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [08:46<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.035272435983642936, 'eval_loss': 1.5558669567108154, 'eval_ap_Adult': 0.0704, 'eval_ap_Calf': 0.0002, 'eval_runtime': 2.8787, 'eval_samples_per_second': 13.201, 'eval_steps_per_second': 1.737, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [09:03<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.03525000101944897, 'eval_loss': 1.5247539281845093, 'eval_ap_Adult': 0.0704, 'eval_ap_Calf': 0.0001, 'eval_runtime': 3.025, 'eval_samples_per_second': 12.562, 'eval_steps_per_second': 1.653, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [09:20<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.034030631446512416, 'eval_loss': 1.5826334953308105, 'eval_ap_Adult': 0.068, 'eval_ap_Calf': 0.0001, 'eval_runtime': 2.7777, 'eval_samples_per_second': 13.68, 'eval_steps_per_second': 1.8, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [09:37<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.03262975066900253, 'eval_loss': 1.539825439453125, 'eval_ap_Adult': 0.0653, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8977, 'eval_samples_per_second': 13.114, 'eval_steps_per_second': 1.726, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "  3%|▎         | 27/810 [09:45<05:21,  2.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8295, 'grad_norm': 43.13096618652344, 'learning_rate': 1.5995556879882246e-05, 'epoch': 18.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [09:53<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.04172024130821228, 'eval_loss': 1.5074447393417358, 'eval_ap_Adult': 0.0834, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.7064, 'eval_samples_per_second': 14.041, 'eval_steps_per_second': 1.848, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [10:11<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.043391164392232895, 'eval_loss': 1.418958306312561, 'eval_ap_Adult': 0.0868, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.9677, 'eval_samples_per_second': 12.805, 'eval_steps_per_second': 1.685, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [10:27<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.04329508584487485, 'eval_loss': 1.4482594728469849, 'eval_ap_Adult': 0.0863, 'eval_ap_Calf': 0.0002, 'eval_runtime': 2.7253, 'eval_samples_per_second': 13.943, 'eval_steps_per_second': 1.835, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [10:44<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.0444614322623238, 'eval_loss': 1.4630017280578613, 'eval_ap_Adult': 0.0879, 'eval_ap_Calf': 0.001, 'eval_runtime': 2.9395, 'eval_samples_per_second': 12.927, 'eval_steps_per_second': 1.701, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [11:01<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.05424254751233093, 'eval_loss': 1.4154812097549438, 'eval_ap_Adult': 0.1085, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8831, 'eval_samples_per_second': 13.18, 'eval_steps_per_second': 1.734, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [11:18<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.053653314820621745, 'eval_loss': 1.3808332681655884, 'eval_ap_Adult': 0.1073, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.7862, 'eval_samples_per_second': 13.639, 'eval_steps_per_second': 1.795, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [11:35<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.052218448370695114, 'eval_loss': 1.3928431272506714, 'eval_ap_Adult': 0.1044, 'eval_ap_Calf': 0.0, 'eval_runtime': 3.0074, 'eval_samples_per_second': 12.636, 'eval_steps_per_second': 1.663, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [11:52<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.05166024714708328, 'eval_loss': 1.414638876914978, 'eval_ap_Adult': 0.1033, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.8699, 'eval_samples_per_second': 13.241, 'eval_steps_per_second': 1.742, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [12:09<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.05174162611365318, 'eval_loss': 1.4169679880142212, 'eval_ap_Adult': 0.1035, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.861, 'eval_samples_per_second': 13.282, 'eval_steps_per_second': 1.748, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [12:26<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.05253266484942287, 'eval_loss': 1.4110380411148071, 'eval_ap_Adult': 0.1041, 'eval_ap_Calf': 0.001, 'eval_runtime': 2.8524, 'eval_samples_per_second': 13.322, 'eval_steps_per_second': 1.753, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [12:42<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.05214706435799599, 'eval_loss': 1.407902479171753, 'eval_ap_Adult': 0.1043, 'eval_ap_Calf': 0.0, 'eval_runtime': 2.7596, 'eval_samples_per_second': 13.77, 'eval_steps_per_second': 1.812, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                              \n",
      "  3%|▎         | 27/810 [13:00<05:21,  2.43it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_ap': 0.05209895223379135, 'eval_loss': 1.4079298973083496, 'eval_ap_Adult': 0.1042, 'eval_ap_Calf': 0.0, 'eval_runtime': 3.0501, 'eval_samples_per_second': 12.459, 'eval_steps_per_second': 1.639, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "100%|██████████| 810/810 [08:23<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 503.803, 'train_samples_per_second': 12.684, 'train_steps_per_second': 1.608, 'train_loss': 1.6614487259476274, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=1.6614487259476274, metrics={'train_runtime': 503.803, 'train_samples_per_second': 12.684, 'train_steps_per_second': 1.608, 'total_flos': 5.002340284130918e+18, 'train_loss': 1.6614487259476274, 'epoch': 30.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
